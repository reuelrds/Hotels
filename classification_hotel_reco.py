# -*- coding: utf-8 -*-
"""Classification_hotel_reco.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fLNn5iAn7qi0_iGyn5wJQbTInMOJJ4Q8

# Hotel Review Sentiment classification

## Loading Data
"""

# !wget wget --header="Host: doc-08-2k-docs.googleusercontent.com" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9" --header="Accept-Language: en-IN,en-US;q=0.9,en-GB;q=0.8,en;q=0.7" --header="Referer: https://mail.google.com/" --header="Cookie: AUTH_lli6c05mt1ep0knhvslord3q0prdrpub_nonce=do8vdecp02mdq" --header="Connection: keep-alive" "https://doc-08-2k-docs.googleusercontent.com/docs/securesc/h8bb16bkfvagme1ou5htdr3n4kh2lejl/71i72ldbbo0iqid8684o6ja91eiphvrc/1629651000000/13390495448190500338/18263537268733356807/1OGaNm2xNSqhszog_pNklSJ2xw88RuzDd?e=download&authuser=0&nonce=do8vdecp02mdq&user=18263537268733356807&hash=2rim36ao0agcofprvhodjttbh36vhsdi" -c -O 'data.zip'
# !unzip /content/data.zip

from google.colab import drive
drive.mount('/content/drive')









"""## Loading data into Pandas"""

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/movie_recommendation/data/sentiment_data.csv",header=None)

df.head()

df.columns = ["Unnamed: 0", "ReviewId","ReviewText","sentiment"]

df = df.drop(columns=["Unnamed: 0"])

df.head()

"""## Model - BERT MODEL - using bert base uncased"""

# using bert base uncased

# !pip install tensorflow_text

# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(df.ReviewText, df.sentiment, test_size=0.2, random_state=42 ,stratify=df["sentiment"])

# import tensorflow as tf
# import tensorflow_hub as hub
# import tensorflow_text as text

# bert_preprocess = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
# bert_encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4")

# def get_sentence_embeding(sentences):
#     preprocessed_text = bert_preprocess(sentences)
#     return bert_encoder(preprocessed_text)['pooled_output']

# get_sentence_embeding(["encoded word"])

# # Bert layers
# text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
# preprocessed_text = bert_preprocess(text_input)
# outputs = bert_encoder(preprocessed_text)

# # Neural network layers
# l = tf.keras.layers.Dropout(0.1, name="dropout")(outputs['pooled_output'])
# l = tf.keras.layers.Dense(1, activation='sigmoid', name="output")(l)

# # Use inputs and outputs to construct a final model
# model = tf.keras.Model(inputs=[text_input], outputs = [l])

# model.summary()

# model.compile(optimizer='adam',
#               loss='binary_crossentropy',
#               metrics=['accuracy'])

# model.fit(X_train, y_train, epochs=3)

"""## Machine learning Model - Logistic regression"""

#******************** machine learning model **************************************

df = df[df['sentiment'] != 0]
df.sentiment.value_counts()

sent = df.sentiment.replace(-1,0)
df['Sentiment'] = sent
df.Sentiment.value_counts()

0, - 1
1, - 0
2, - 1
3, - 0
4, - 1
5, - 0

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.ReviewText, df.Sentiment, test_size=0.2, random_state=42 ,stratify=df["Sentiment"])

print(X_train[:3])

from tqdm import tqdm
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
stopword = set(stopwords.words('english'))
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
def data_cleaner(data):
    clean_data = []
    for sentence in tqdm(data):
        cleantext = BeautifulSoup(sentence, "lxml").text #html tags
        cleantext = re.sub(r'[^\w\s]','',cleantext) # punctuation
        cleantext = [token for token in cleantext.lower().split() if token not in stopword] #stopword
        clean_text = ' '.join([lemmatizer.lemmatize(token) for token in cleantext])
        clean_data.append(clean_text.strip())
    return clean_data

clean_data_train_data = data_cleaner(X_train.values)
clean_data_test_data =  data_cleaner(X_test.values)

print(clean_data_train_data[:3])

"""#### TFIDF Vectorizer"""

[i am tapan]
[0.3 0.5 1.0]

from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer()
vec.fit(clean_data_train_data)
train_x_tfidf = vec.transform(clean_data_train_data)
test_x_tfidf = vec.transform(clean_data_test_data)

print(train_x_tfidf.shape)
print(test_x_tfidf.shape)

print(train_x_tfidf[:1])

"""## BaseLine model"""

MODEL_LR = "LogisticRegression"
MODEL_NB = "MultinomialNB"
MODEL_RF = "RandomForestClassifier"

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score,classification_report

def train_model(model_name):

    if model_name == MODEL_LR:
        print("Training With LogisticRegression..")
        lr = LogisticRegression()
        lr.fit(train_x_tfidf,y_train)
        # [0.1 0.34 0.55] [1] 
        # [0.1 0.34 0.55] [1] 
        # [0.1 0.34 0.55] [0] -> 40k
        predict = lr.predict(test_x_tfidf)
        # [0.4 0.34 0.55]        lr.pred -> [1]                             | [1] - 100%
        # [0.7 0.34 0.55]        lr.pred -> [0]                             | [1] - 0%
        # [0.0 0.14 0.55] -> 10k lr.pred -> [1] _ y_predicted_with_my_model | [1] - 100%
        print()
        acc = roc_auc_score(y_test,predict)
        print("AUC is :\033[1m  {:.2f}%  \033[0m".format(acc*100))
        print("F1 score: {:.2f}".format(f1_score(y_test,predict, average='weighted')))
        print()
        print("Report:")
        print("Classification report :",classification_report(y_test,predict))
    elif model_name == MODEL_NB:
        print("Training With MultinomialNB..")
        nb = MultinomialNB()
        nb.fit(train_x_tfidf,y_train)
        predict = nb.predict(test_x_tfidf)
        print()
        acc = roc_auc_score(y_test,predict)
        print("AUC is :\033[1m  {:.2f}%  \033[0m".format(acc*100))
        print("F1 score: {:.2f}".format(f1_score(y_test,predict, average='weighted')))
        print()
        print("Report:")
        print("Classification report :",classification_report(y_test,predict))
    elif model_name == MODEL_RF:
        print("Training With RandomForestClassifier..")
        rf = RandomForestClassifier()
        rf.fit(train_x_tfidf,y_train)
        predict = rf.predict(test_x_tfidf)
        print()
        acc = roc_auc_score(y_test,predict)
        print("AUC is :\033[1m  {:.2f}%  \033[0m".format(acc*100))
        print("F1 score: {:.2f}".format(f1_score(y_test,predict, average='weighted')))
        print()
        print("Report:")
        print("Classification report :",classification_report(y_test,predict))
    else:
        print("Model didn't loaded properly.")

train_model(MODEL_LR)

train_model(MODEL_NB)

train_model(MODEL_RF)



"""## Training with Hyperparameter tuning -- Logistic Regression

---


"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
lr = LogisticRegression(class_weight='balanced')



c_ranges = {"C":[10**-2,10**-1,10**0,10**1,10**2]}
grid_search =GridSearchCV(lr , param_grid = c_ranges ,scoring= "roc_auc",cv=3,return_train_score = True, verbose=10)
grid_search.fit(train_x_tfidf,y_train)

grid_search.best_estimator_

"""##### Training Logistic with Best parameter"""

lr = LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
lr.fit(train_x_tfidf,y_train)

predict = lr.predict(test_x_tfidf)



print("AUC is :",roc_auc_score(y_test,predict))

print("Classification report :",classification_report(y_test,predict))

"""##### Plot ROC AUC Curve"""

from sklearn import metrics
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.metrics import roc_curve, auc

y_pred_proba = lr.predict_proba(test_x_tfidf)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)

fig = px.area(
    x=fpr, y=tpr,
    title=f'ROC Curve (AUC={auc})',
    labels=dict(x='False Positive Rate', y='True Positive Rate'),
    width=700, height=500
)
fig.add_shape(
    type='line', line=dict(dash='dash'),
    x0=0, x1=1, y0=0, y1=1
)

fig.update_yaxes(scaleanchor="x", scaleratio=1)
fig.update_xaxes(constrain='domain')
fig.show()







"""## Training with Hyperparameter tuning - Naive Bayes

---


"""

from sklearn.model_selection import GridSearchCV

# lr = LogisticRegression(class_weight='balanced')
nb = MultinomialNB(class_prior=None)

nomialnb_parameters = {"alpha":[10**-2,10**-1,10**0,10**1,10**2], "fit_prior":[True, False]}

grid_search_nb = GridSearchCV(nb , param_grid = nomialnb_parameters ,scoring= "roc_auc",cv=3,return_train_score = True, verbose=10)
grid_search_nb.fit(train_x_tfidf,y_train)

grid_search_nb.best_estimator_

nb_tuned = MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
nb_tuned.fit(train_x_tfidf,y_train)

predict_nb = nb_tuned.predict(test_x_tfidf)

print("AUC is :",roc_auc_score(y_test,predict_nb))

print("Classification report :",classification_report(y_test,predict_nb))

"""## Machine learning Model - Random Forest"""

# lr = LogisticRegression(class_weight='balanced')
from sklearn.model_selection import RandomizedSearchCV
rf = RandomForestClassifier()

# nomialnb_parameters = {"alpha":[10**-2,10**-1,10**0,10**1,10**2], "fit_prior":[True, False]}
randomforest_parameters = {'bootstrap': [True, False],
 'max_depth': [1, 2, 5, 8, 10],
 'max_features': ['auto', 'sqrt', 'log2'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [50, 100, 200, 500, 1000]}#[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}

#  n_estimators = [100, 300, 500, 800, 1200]
# max_depth = [5, 8, 15, 25, 30]
# min_samples_split = [2, 5, 10, 15, 100]
# min_samples_leaf = [1, 2, 5, 10] 

# hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  
#               min_samples_split = min_samples_split, 
#              min_samples_leaf = min_samples_leaf)

random_search_rf = RandomizedSearchCV(rf, randomforest_parameters, random_state=0 )#,scoring= "roc_auc",cv=3,return_train_score = True, verbose=10)
random_search_rf.fit(train_x_tfidf,y_train)

random_search_rf.best_params_

rf_tuned = RandomForestClassifier()
rf_tuned.fit(train_x_tfidf,y_train)

predict_rf = rf_tuned.predict(test_x_tfidf)

print("AUC is :",roc_auc_score(y_test,predict_rf))

print("Classification report :",classification_report(y_test,predict_rf))



